# Single stack: start/stop everything with: docker compose up -d | docker compose down
# Project name "olo" isolates this stack from others (containers, network, volumes).
name: olo

services:
  olo-postgres:
    image: postgres:15
    container_name: olo-postgres
    environment:
      POSTGRES_USER: temporal
      POSTGRES_PASSWORD: pgpass
      POSTGRES_DB: temporal
    ports:
      - "5432:5432"
    networks:
      - olo-net
    restart: unless-stopped
      
  elasticsearch:
    container_name: olo-elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION:-7.17.23}
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=512mb
      - cluster.routing.allocation.disk.watermark.high=256mb
      - cluster.routing.allocation.disk.watermark.flood_stage=128mb
      - ES_JAVA_OPTS=-Xms256m -Xmx256m
    ports:
      - "9200:9200"
    volumes:
      - olo-elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - olo-net
    restart: unless-stopped

  kibana:
    container_name: olo-kibana
    image: docker.elastic.co/kibana/kibana:${ELASTICSEARCH_VERSION:-7.17.23}
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana
      - SERVER_PUBLICBASEURL=http://localhost:5601
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - olo-net
    restart: unless-stopped
  temporal:
    container_name: olo-temporal
    image: temporalio/auto-setup:${TEMPORAL_VERSION:-1.22.4}
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=pgpass
      - POSTGRES_SEEDS=olo-postgres
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development-sql.yaml
      - ENABLE_ES=true
      - ES_SEEDS=elasticsearch
      - ES_VERSION=v7
      - PROMETHEUS_ENDPOINT=0.0.0.0:8000
    ports:
      - "7233:7233"
      - "8000:8000"
    volumes:
      - ./dynamicconfig:/etc/temporal/config/dynamicconfig
    networks:
      - olo-net
    restart: unless-stopped

  temporal-admin-tools:
    container_name: olo-temporal-admin-tools
    image: temporalio/admin-tools:${TEMPORAL_ADMINTOOLS_VERSION:-1.22.4}
    depends_on:
      - temporal
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CLI_ADDRESS=temporal:7233
    stdin_open: true
    tty: true
    networks:
      - olo-net
    restart: unless-stopped

  temporal-ui:
    container_name: olo-temporal-ui
    image: temporalio/ui:${TEMPORAL_UI_VERSION:-2.22.2}
    depends_on:
      - temporal
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000
    ports:
      - "8080:8080"
    networks:
      - olo-net
    restart: unless-stopped

  # ----------------------------
  # Vector DB (Qdrant)
  # ----------------------------
  qdrant:
    container_name: olo-qdrant
    image: qdrant/qdrant:${QDRANT_VERSION:-v1.12.0}
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - olo-qdrant_data:/qdrant/storage
    networks:
      - olo-net
    restart: unless-stopped

  # ----------------------------
  # OSS models (Ollama)
  # ----------------------------
  ollama:
    container_name: olo-ollama
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - olo-ollama_data:/root/.ollama
    networks:
      - olo-net
    restart: unless-stopped
    # GPU: requires NVIDIA driver + nvidia-container-toolkit. Remove deploy block to run on CPU only.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ----------------------------
  # OpenAI OSS (LiteLLM proxy – OpenAI-compatible API over Ollama)
  # ----------------------------
  openai-oss:
    container_name: olo-openai-oss
    image: docker.litellm.ai/berriai/litellm:main-latest
    ports:
      - "4000:4000"
    volumes:
      - ./litellm/config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml"]
    depends_on:
      - ollama
    networks:
      - olo-net
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ----------------------------
  # Redis + Redis Insight
  # ----------------------------
  redis:
    container_name: olo-redis
    image: redis:${REDIS_VERSION:-7-alpine}
    ports:
      - "6379:6379"
    volumes:
      - olo-redis_data:/data
    networks:
      - olo-net
    restart: unless-stopped
    command: redis-server --appendonly yes

  redis-insight:
    container_name: olo-redis-insight
    image: redis/redisinsight:${REDIS_INSIGHT_VERSION:-latest}
    ports:
      - "5540:5540"
    volumes:
      - olo-redis-insight_data:/data
    depends_on:
      - redis
    networks:
      - olo-net
    restart: unless-stopped

  # ----------------------------
  # Open LLM Orchestrator Worker (Temporal worker: runs CoreWorkflow, etc.)
  # ----------------------------
  open-llm-orchestrator-worker:
    image: openllmorchestrator/open-llm-orchestrator-worker:${WORKER_IMAGE_TAG:-latest}
    container_name: olo-worker
    restart: unless-stopped
    depends_on:
      - temporal
      - ollama
      - qdrant
      - redis
    networks:
      - olo-net
    environment:
      # Temporal (worker expects TEMPORAL_TARGET; some clients use TEMPORAL_ADDRESS)
      TEMPORAL_TARGET: ${TEMPORAL_TARGET:-temporal:7233}
      TEMPORAL_ADDRESS: ${TEMPORAL_ADDRESS:-temporal:7233}
      TEMPORAL_NAMESPACE: ${TEMPORAL_NAMESPACE:-default}
      QUEUE_NAME: ${QUEUE_NAME:-core-task-queue}
      # Config loading (Redis → DB → file)
      CONFIG_KEY: ${CONFIG_KEY:-default}
      CONFIG_VERSION: ${CONFIG_VERSION:-1.0}
      CONFIG_RETRY_SLEEP_SECONDS: ${CONFIG_RETRY_SLEEP_SECONDS:-30}
      # Redis (config storage / cache)
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      # Database (config storage; create DB olo_config on postgres if used)
      DB_URL: ${WORKER_DB_URL:-jdbc:postgresql://olo-postgres:5432/olo_config}
      DB_USERNAME: ${WORKER_DB_USERNAME:-temporal}
      DB_PASSWORD: ${WORKER_DB_PASSWORD:-pgpass}
      # Worker tuning
      MAX_CONCURRENT_WORKFLOW_TASK_POLLERS: ${MAX_CONCURRENT_WORKFLOW_TASK_POLLERS:-5}
      MAX_CONCURRENT_ACTIVITY_TASK_POLLERS: ${MAX_CONCURRENT_ACTIVITY_TASK_POLLERS:-10}
      # Ollama (LLM pipelines)
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      OLLAMA_URL: ${OLLAMA_URL:-http://ollama:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.2:latest}
      OLLAMA_TIMEOUT_SECONDS: ${OLLAMA_TIMEOUT_SECONDS:-300}
      # OpenAI-compatible API (LiteLLM) and Qdrant
      OPENAI_API_BASE: ${OPENAI_API_BASE:-http://openai-oss:4000}
      QDRANT_URL: ${QDRANT_URL:-http://qdrant:6333}
    env_file:
      - .env
    volumes:
      - ${WORKER_SHARED_VOLUME:-./shared}:/app/shared

  olo-ui:
    image: openllmorchestrator/open-llm-orchestrator-ui:latest
    container_name: olo-ui
    restart: unless-stopped
    ports:
      - "${PORT_BACKEND:-8002}:8002"
      - "${PORT_UI:-5173}:5173"
    networks:
      - olo-net
    environment:
      NODE_ENV: production
      PORT: 8002
      TEMPORAL_ADDRESS: ${TEMPORAL_ADDRESS:-temporal:7233}
      TEMPORAL_NAMESPACE: ${TEMPORAL_NAMESPACE:-default}
      TEMPORAL_TASK_QUEUE: ${TEMPORAL_TASK_QUEUE:-core-task-queue}
      TEMPORAL_CHAT_WORKFLOW: ${TEMPORAL_CHAT_WORKFLOW:-CoreWorkflow}
      TEMPORAL_WORKFLOW_ID_TEMPLATE: ${TEMPORAL_WORKFLOW_ID_TEMPLATE:-chat-{{pipelineId}}-{{timestamp}}}
      TEMPORAL_WORKFLOW_CLASS: ${TEMPORAL_WORKFLOW_CLASS:-core-task-queue}
      TEMPORAL_DOC_WORKFLOW: ${TEMPORAL_DOC_WORKFLOW:-CoreWorkflow}
      TEMPORAL_DOC_WORKFLOW_ID_TEMPLATE: ${TEMPORAL_DOC_WORKFLOW_ID_TEMPLATE:-doc-ingest-{{ragTag}}-{{timestamp}}}
      TEMPORAL_DOC_TASK_QUEUE: ${TEMPORAL_DOC_TASK_QUEUE:-core-task-queue}
      USE_STUB_LLM: ${USE_STUB_LLM:-0}
      TEMPLATES_DIR: ${TEMPLATES_DIR:-/mnt/templates}
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      STORE_FILE: ${STORE_FILE:-}
      PIPELINE_OPTIONS: ${PIPELINE_OPTIONS:-}
      PIPELINE_OPTIONS_RAG: ${PIPELINE_OPTIONS_RAG:-}
    env_file:
      - .env
    volumes:
      # Pipeline templates (chat/upload .tpl files)
      - ${TEMPLATES_VOLUME:-./templates-example}:/mnt/templates:ro
      # Optional: persist file store when not using Redis (ensure /app/data is writable by container user)
      # - app-data:/app/data
    healthcheck:
      test: [ "CMD", "node", "-e", "fetch('http://127.0.0.1:8002/api/health').then(r=>process.exit(r.ok?0:1)).catch(()=>process.exit(1))" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 128M
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ----------------------------------------
  # 1. Stable Diffusion (Automatic1111)
  # ----------------------------------------
  stable-diffusion:
    image: aidockorg/stable-diffusion-webui-cuda:latest
    container_name: stable-diffusion
    ports:
      - "7860:7860"
    volumes:
      - ./workspace-stable-diffusion:/workspace
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - WEBUI_ARGS=--xformers --api
      - WEB_ENABLE_AUTH=false
    restart: unless-stopped
    networks:
      - olo-net

  # ----------------------------------------
  # 2. ComfyUI
  # ----------------------------------------
  comfyui:
    image: aidockorg/comfyui-cuda:latest
    container_name: comfyui
    ports:
      - "8188:8188"
    volumes:
      - ./workspace-comfyui:/workspace
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - WEB_ENABLE_AUTH=false
    restart: unless-stopped
    networks:
      - olo-net

  # ----------------------------------------
  # 3. InvokeAI
  # ----------------------------------------
  invokeai:
    image: ghcr.io/invoke-ai/invokeai:latest
    container_name: invokeai
    ports:
      - "9090:9090"
    volumes:
      - ./models/invokeai:/invokeai
      - ./outputs/invokeai:/invokeai/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    restart: unless-stopped
    networks:
      - olo-net
  olo-config:
    image: openllmorchestrator/olo-config:latest
    container_name: olo-config
    depends_on:
      - olo-postgres
      - redis
    networks:
      - olo-net
    ports:
      - "${SERVER_PORT:-8082}:8082"
    environment:
      SERVER_ADDRESS: 0.0.0.0
      SPRING_DATASOURCE_URL: jdbc:postgresql://${POSTGRES_HOST:-olo-postgres}:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-temporal}
      SPRING_DATASOURCE_DRIVER: org.postgresql.Driver
      SPRING_DATASOURCE_USERNAME: ${POSTGRES_USER:-temporal}
      SPRING_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD:-pgpass}
      SPRING_DATA_REDIS_HOST: ${SPRING_DATA_REDIS_HOST:-redis}
      SPRING_DATA_REDIS_PORT: ${SPRING_DATA_REDIS_PORT:-6379}
      SPRING_DATA_REDIS_PASSWORD: ${SPRING_DATA_REDIS_PASSWORD:-}
      OLO_TEMPLATES_DIR: ${OLO_TEMPLATES_DIR:-/app/template}
      OLO_COMPONENTS_DIR: ${OLO_COMPONENTS_DIR:-/app/components}
      OLO_PLUGINS_DIR: ${OLO_PLUGINS_DIR:-/app/components/plugins}
      OLO_REDIS_CONFIG_KEY_PREFIX: ${OLO_REDIS_CONFIG_KEY_PREFIX:-olo:config:}
      OLO_REDIS_ENGINE_CONFIG_KEY_PREFIX: ${OLO_REDIS_ENGINE_CONFIG_KEY_PREFIX:-olo:engine:config:}
      OLO_REDIS_ENABLED: ${OLO_REDIS_ENABLED:-true}
      SPRING_JPA_HIBERNATE_DDL_AUTO: ${SPRING_JPA_HIBERNATE_DDL_AUTO:-update}
    volumes:
      - ./template:/app/template:ro
      - ./components:/app/components:ro
    restart: unless-stopped
# ----------------------------
# Network
# ----------------------------
networks:
  olo-net:
    driver: bridge

# ----------------------------
# Volumes
# ----------------------------
volumes:
  olo-ollama_data:
  olo-elasticsearch-data:
  olo-qdrant_data:
  olo-redis_data:
  olo-redis-insight_data: